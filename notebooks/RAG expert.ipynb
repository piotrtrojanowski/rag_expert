{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518ee296-82ac-4f55-a559-cc6154aa4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771af2c9-c157-4662-8d0d-a10ba5245a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama2\")\n",
    "# Example usage\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c080af-a4f5-41c5-bf4e-ba6752db61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_summary(pdf_reader):\n",
    "    try:\n",
    "        info = pdf_reader.metadata\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        summary = {\n",
    "            \"title\": info.title,\n",
    "            \"author\": info.author,\n",
    "            \"creator\": info.creator,\n",
    "            \"producer\": info.producer,\n",
    "            \"subject\": info.subject,\n",
    "            \"num_pages\": num_pages,\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_pdf_summary(summary):\n",
    "    if summary:\n",
    "        print(\"PDF Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            if value: #Check if value is not None\n",
    "                print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve PDF summary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff5bd5-b696-424e-8424-9154034d9aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader=PdfReader(\"data/SteveJobs autobiography book.pdf\")\n",
    "# steve-jobs-stanford-university-commencement-speech.pdf\n",
    "summary = get_pdf_summary(pdfReader)\n",
    "print_pdf_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042e21e-1a4d-4720-9fa8-0d21d4e1be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data\"\n",
    "vector_store_directory = \"vector_store\"\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3ca2f-5cd4-4b41-8efd-5054e495ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only necessary before the vector store was created\n",
    "def choose_embedding_function ():\n",
    "    # Initialize the embedding function\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bd2ec0-2f9b-477c-98f3-acb6e5a494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter    \n",
    "\n",
    "def load_or_create_chroma(\n",
    "    pdfReader: PdfReader,\n",
    "    vector_store_directory: str\n",
    ") -> Chroma:\n",
    "    \"\"\"Loads or creates a Chroma vector store efficiently.\"\"\"\n",
    "    db_file = os.path.join(vector_store_directory, \"chroma.sqlite3\")\n",
    "    try:\n",
    "        if os.path.exists(db_file):\n",
    "            print(f\"Loading existing Chroma vector store from: {vector_store_directory}\")\n",
    "            vector_store = Chroma(persist_directory=vector_store_directory)\n",
    "            \n",
    "            '''if (vector_store._embedding_function.__class__ != embedding_function.__class__):\n",
    "                print(\"Embedding function has changed. Recreating vectorstore.\")\n",
    "                shutil.rmtree(vector_store_directory)\n",
    "                raise FileNotFoundError #to trigger recreation'''\n",
    "        else:\n",
    "            print(f\"db file does not exist in: {db_file}\")\n",
    "            raise FileNotFoundError #to trigger creation\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Received request to recreate vector store. Creating new Chroma vector store in: {vector_store_directory}\")\n",
    "\n",
    "        raw_text=''\n",
    "        for i, page in enumerate(pdfReader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                raw_text += text\n",
    "    \n",
    "        if not raw_text:\n",
    "            print(\"raw_text is empty. Cannot create a vector store.\")\n",
    "    \n",
    "        textSplitter = RecursiveCharacterTextSplitter (\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=20,\n",
    "            length_function=len\n",
    "        )\n",
    "        textChunks = textSplitter.split_text(raw_text)\n",
    "            \n",
    "        vector_store = Chroma.from_texts(\n",
    "            texts=textChunks,\n",
    "            embedding=choose_embedding_function(),\n",
    "            persist_directory=vector_store_directory\n",
    "        )\n",
    "\n",
    "        print(f\"Persisting new Chroma vector store to: {vector_store_directory}\")\n",
    "        try:\n",
    "            vector_store.persist()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with the Chroma vector store: {e}\")\n",
    "            raise\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd648ab-38dd-474b-bbf2-908029ea3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vector_store_summary(vector_store: Chroma):\n",
    "    \"\"\"Displays a summary of the Chroma vector store, including embedding function info.\"\"\"\n",
    "    try:\n",
    "        collection = vector_store._collection  # Access the underlying collection\n",
    "        count = collection.count()\n",
    "        print(f\"Vector store contains {count} embeddings.\")\n",
    "\n",
    "        # Display embedding function information\n",
    "        embedding_function = vector_store._embedding_function\n",
    "        print(\"Embedding Function:\")\n",
    "        print(f\"  Type: {type(embedding_function).__name__}\")\n",
    "        if hasattr(embedding_function, \"model_name\"):\n",
    "            print(f\"  Model Name: {embedding_function.model_name}\")\n",
    "        elif hasattr(embedding_function, \"model\"): #For sentence transformers\n",
    "            print(f\"  Model: {embedding_function.model}\")\n",
    "        # Add other relevant embedding function attributes as needed\n",
    "\n",
    "        metadata = collection.metadata\n",
    "        if metadata:\n",
    "            print(\"Metadata:\")\n",
    "            for key, value in metadata.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "        else:\n",
    "          print(\"No metadata available\")\n",
    "\n",
    "        if count > 0:\n",
    "            peek = collection.peek()\n",
    "            print(\"First few documents:\")\n",
    "            for doc in peek['documents'][:min(5, count)]:\n",
    "                print(f\"  {doc[:100]}...\")\n",
    "        else:\n",
    "            print(\"No documents available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying vector store summary: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12bda76-a425-4664-81da-c6d8998d3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming you have defined choose_embedding_function and textChunks):\n",
    "try:\n",
    "    vector_store = load_or_create_chroma(pdfReader, vector_store_directory)\n",
    "    display_vector_store_summary(vector_store)\n",
    "except ValueError as e:\n",
    "        print(f\"A value error occured: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36037434-cb21-4ffe-8097-e039407cd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045100e-e4bd-435f-a09e-4a3503c00e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "\n",
    "pdf_qa_template = \"\"\"Use the following context to answer the question at the end. If you don't know the answer based on the context, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(template=pdf_qa_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "pdf_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs={\"prompt\": QA_PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eedf78-7cdc-4541-8651-8801ca66c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 1: only PDF as a source\n",
    "# 5. Ask a question\n",
    "query = \"What can you say about Steve Jobs style of management?\"\n",
    "result = pdf_qa.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0832b479-1d35-47a3-b9f4-a78e7b70f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2: FULL VERSION - Three Sources - the fully blown approach: leverage PDFs + LLM + Internet\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "#from langchain.tools import SerpAPIWrapper # Import SerpAPIWrapper explicitly\n",
    "import os\n",
    "\n",
    "# 1. Load from PDFs\n",
    "# This shows how to generalize to a list of pdfs :\n",
    "''' pdf_paths = [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"]  # Replace with your PDF paths\n",
    "pdf_docs = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pdf_docs.extend(loader.load())\n",
    "'''\n",
    "# 2. Split text from PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 3. Create vectorstore from PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 4. Initialize LLM (for both internal knowledge and final answer generation)\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 5. Initialize the internet searches - SerpAPI - a tool for internet search (Crucially with the wrapper)\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "serpapi_api_key = os.environ.get(\"SERPAPI_API_KEY\") #does not work, so workaround below\n",
    "SERPAPI_API_KEY='49c6ecf880ecddcca49c7795464d7d235a932823cba6f66afe70c01e91383536'\n",
    "serpapi_api_key=SERPAPI_API_KEY\n",
    "if serpapi_api_key:\n",
    "    internet_search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
    "else:\n",
    "    print(\"SERPAPI_API_KEY environment variable not set. Internet searches will be unavailable.\")\n",
    "    internet_search = [] # search is unavailable if no API key\n",
    "\n",
    "# 6. Define RetrievalQA chain for PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code her\n",
    "#QA_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "#pdf_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=pdf_retriever, chain_type_kwargs={\"prompt\": QA_PROMPT})\n",
    "\n",
    "\n",
    "def get_context(query):\n",
    "    pdf_context = pdf_qa.invoke({\"query\": query})['result']\n",
    "#    print(f\"PDF context: {pdf_context}\")\n",
    "    llm_context = llm.invoke(query).content\n",
    "#    print(f\"LLM context: {llm_context}\")\n",
    "    internet_context = internet_search.run(query) if internet_search else \"Internet search is unavailable.\" # Direct SerpAPI use\n",
    "#    print(f\"Internet context: {internet_context}\")\n",
    "    combined_context = f\"PDF Context:\\n{pdf_context}\\n\\nLLM Context:\\n{llm_context}\\n\\nInternet Context:\\n{internet_context}\"\n",
    "#    print(combined_context)\n",
    "    return combined_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5e92b-e67f-4e76-9438-57f94afa1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Final prompt template and chain\n",
    "final_template = \"\"\"Use the combined context below from PDFs, an LLM, and the internet to answer the question. Treat the three contexts as three sources of information. If the information is not present in the context say I don't know.\n",
    "\n",
    "Combined Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    template=final_template, input_variables=[\"combined_context\", \"question\"]\n",
    ")\n",
    "\n",
    "final_chain = LLMChain(llm=llm, prompt=FINAL_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d76100-9d8a-42ae-9d24-3aed514f4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ask a question\n",
    "query = \"How many children did Steve Jobs have?\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bfa9d5-0da2-4013-b2ae-858c46dfda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ask a question\n",
    "query = \"List names and birthdates of all Steve Jobs's children\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ff9c1-a903-4f27-bb52-f3a27beb7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ask a question\n",
    "query = \"What can you say about Steve Jobs style of management?\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07a6981-6641-45c1-be4a-327b47c7f40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
