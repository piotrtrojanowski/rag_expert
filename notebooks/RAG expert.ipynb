{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518ee296-82ac-4f55-a559-cc6154aa4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5eb0a0-f0c4-43a8-86f4-17fe7bede408",
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.14\n",
      "0.2.2\n",
      "Name: chromadb\n",
      "Version: 0.5.3\n",
      "Summary: Chroma.\n",
      "Home-page: https://github.com/chroma-core/chroma\n",
      "Author: \n",
      "Author-email: Jeff Huber <jeff@trychroma.com>, Anton Troynikov <anton@trychroma.com>\n",
      "License: \n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: bcrypt, build, chroma-hnswlib, fastapi, grpcio, httpx, importlib-resources, kubernetes, mmh3, numpy, onnxruntime, opentelemetry-api, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, opentelemetry-sdk, orjson, overrides, posthog, pydantic, pypika, PyYAML, requests, tenacity, tokenizers, tqdm, typer, typing-extensions, uvicorn\n",
      "Required-by: langchain-chroma\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)\n",
    "\n",
    "import langchain_ollama\n",
    "print(langchain_ollama.__version__)\n",
    "\n",
    "!pip show chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f28c6-4df8-4d42-a1bc-38325cea8f76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca2a44-91e2-4603-a550-20fda769c5de",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "!pip list > jupyter_packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af4b10c-bc8e-4b34-bb71-0cafed5ff58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "771af2c9-c157-4662-8d0d-a10ba5245a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6w/77j1g1r91fqfc5x3qqx3mrdw0000gn/T/ipykernel_24250/3818618631.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOllama\n",
    "llm = ChatOllama(model=\"llama2\")\n",
    "# Example usage\n",
    "response = llm.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c080af-a4f5-41c5-bf4e-ba6752db61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_summary(pdf_reader):\n",
    "    try:\n",
    "        info = pdf_reader.metadata\n",
    "        num_pages = len(pdf_reader.pages)\n",
    "\n",
    "        summary = {\n",
    "            \"title\": info.title,\n",
    "            \"author\": info.author,\n",
    "            \"creator\": info.creator,\n",
    "            \"producer\": info.producer,\n",
    "            \"subject\": info.subject,\n",
    "            \"num_pages\": num_pages,\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "def print_pdf_summary(summary):\n",
    "    if summary:\n",
    "        print(\"PDF Summary:\")\n",
    "        for key, value in summary.items():\n",
    "            if value: #Check if value is not None\n",
    "                print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(\"Could not retrieve PDF summary.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecff5bd5-b696-424e-8424-9154034d9aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Summary:\n",
      "  title: Steve Jobs' Stanford University Commencement Speech\n",
      "  producer: Prince 20160408 (www.princexml.com)\n",
      "  num_pages: 7\n"
     ]
    }
   ],
   "source": [
    "#pdfReader=PdfReader(\"../data/SteveJobs-autobiography-book.pdf\")\n",
    "pdfReader=PdfReader(\"../data/steve-jobs-stanford-university-commencement-speech.pdf\")\n",
    "summary = get_pdf_summary(pdfReader)\n",
    "print_pdf_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2042e21e-1a4d-4720-9fa8-0d21d4e1be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"../data\"\n",
    "vector_store_directory = \"../vector_store\"\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf3ca2f-5cd4-4b41-8efd-5054e495ca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only necessary before the vector store was created\n",
    "def choose_embedding_function ():\n",
    "    # Initialize the embedding function\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92bd2ec0-2f9b-477c-98f3-acb6e5a494e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.base import Embeddings\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter    \n",
    "\n",
    "def load_or_create_chroma(\n",
    "    pdfReader: PdfReader,\n",
    "    vector_store_directory: str\n",
    ") -> Chroma:\n",
    "    \"\"\"Loads or creates a Chroma vector store efficiently.\"\"\"\n",
    "    db_file = os.path.join(vector_store_directory, \"chroma.sqlite3\")\n",
    "    try:\n",
    "        if os.path.exists(db_file):\n",
    "            print(f\"Loading existing Chroma vector store from: {vector_store_directory}\")\n",
    "            vector_store = Chroma(persist_directory=vector_store_directory)\n",
    "            vector_store._embedding_function = choose_embedding_function()\n",
    "            '''if (vector_store._embedding_function.__class__ != embedding_function.__class__):\n",
    "                print(\"Embedding function has changed. Recreating vectorstore.\")\n",
    "                shutil.rmtree(vector_store_directory)\n",
    "                raise FileNotFoundError #to trigger recreation'''\n",
    "        else:\n",
    "            print(f\"db file does not exist in: {db_file}\")\n",
    "            raise FileNotFoundError #to trigger creation\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Received request to recreate vector store. Creating new Chroma vector store in: {vector_store_directory}\")\n",
    "\n",
    "        raw_text=''\n",
    "        for i, page in enumerate(pdfReader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                raw_text += text\n",
    "    \n",
    "        if not raw_text:\n",
    "            print(\"raw_text is empty. Cannot create a vector store.\")\n",
    "    \n",
    "        textSplitter = RecursiveCharacterTextSplitter (\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "            chunk_size=500,\n",
    "            chunk_overlap=20,\n",
    "            length_function=len\n",
    "        )\n",
    "        textChunks = textSplitter.split_text(raw_text)\n",
    "            \n",
    "        vector_store = Chroma.from_texts(\n",
    "            texts=textChunks,\n",
    "            embedding=choose_embedding_function(),\n",
    "            persist_directory=vector_store_directory\n",
    "        )\n",
    "\n",
    "        print(f\"Persisting new Chroma vector store to: {vector_store_directory}\")\n",
    "        try:\n",
    "            vector_store.persist()\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred with the Chroma vector store: {e}\")\n",
    "            raise\n",
    "        \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd648ab-38dd-474b-bbf2-908029ea3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_vector_store_summary(vector_store: Chroma):\n",
    "    \"\"\"Displays a summary of the Chroma vector store, including embedding function info.\"\"\"\n",
    "    try:\n",
    "        collection = vector_store._collection  # Access the underlying collection\n",
    "        count = collection.count()\n",
    "        print(f\"Vector store contains {count} embeddings.\")\n",
    "\n",
    "        # Display embedding function information\n",
    "        embedding_function = vector_store._embedding_function\n",
    "        print(\"Embedding Function:\")\n",
    "        print(f\"  Type: {type(embedding_function).__name__}\")\n",
    "        if hasattr(embedding_function, \"model_name\"):\n",
    "            print(f\"  Model Name: {embedding_function.model_name}\")\n",
    "        elif hasattr(embedding_function, \"model\"): #For sentence transformers\n",
    "            print(f\"  Model: {embedding_function.model}\")\n",
    "        # Add other relevant embedding function attributes as needed\n",
    "\n",
    "        if hasattr(collection, \"metadata\"):\n",
    "            metadata = collection.metadata\n",
    "            if metadata:\n",
    "                print(\"Metadata:\")\n",
    "                for key, value in metadata.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "              print(\"No metadata available\")\n",
    "\n",
    "        if count > 0:\n",
    "            peek = collection.peek()\n",
    "            print(\"First few documents:\")\n",
    "            for doc in peek['documents'][:min(5, count)]:\n",
    "                print(f\"  {doc[:100]}...\")\n",
    "        else:\n",
    "            print(\"No documents available\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying vector store summary: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12bda76-a425-4664-81da-c6d8998d3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing Chroma vector store from: ../vector_store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6w/77j1g1r91fqfc5x3qqx3mrdw0000gn/T/ipykernel_24250/1680301023.py:17: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vector_store = Chroma(persist_directory=vector_store_directory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store contains 36 embeddings.\n",
      "Embedding Function:\n",
      "  Type: OllamaEmbeddings\n",
      "  Model: mxbai-embed-large\n",
      "No metadata available\n",
      "First few documents:\n",
      "  Macintosh — a year earlier, and I had just turned 30. And then I got fired. How can you get fired fr...\n",
      "  your gut, destiny, life, karma, whatever. This approach has never let me down, and it has made all t...\n",
      "  him. So at 30 I was out. And very publicly out. What had been the focus of my entire adult life was\n",
      "...\n",
      "  Name: Class:\n",
      "\"Steve Jobs 1955-2011\" by segagman is licensed under CC BY 2.0.Steve Jobs' Stanford Uni...\n",
      "  I even thought about running away from the valley. But something slowly began to dawn on me — I\n",
      "stil...\n"
     ]
    }
   ],
   "source": [
    "# Example usage (assuming you have defined choose_embedding_function and textChunks):\n",
    "try:\n",
    "    vector_store = load_or_create_chroma(pdfReader, vector_store_directory)\n",
    "    display_vector_store_summary(vector_store)\n",
    "except ValueError as e:\n",
    "        print(f\"A value error occured: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36037434-cb21-4ffe-8097-e039407cd8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a045100e-e4bd-435f-a09e-4a3503c00e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "\n",
    "pdf_qa_template = \"\"\"Use the following context to answer the question at the end. If you don't know the answer based on the context, just say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "QA_PROMPT = PromptTemplate(template=pdf_qa_template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "pdf_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs={\"prompt\": QA_PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17eedf78-7cdc-4541-8651-8801ca66c862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the context provided, I can answer the following question:\n",
      "\n",
      "What can you say about Steve Jobs' style of management?\n",
      "\n",
      "Steve Jobs' management style is portrayed as unconventional and visionary in his Stanford University Commencement Speech. He believes in taking risks and investing in people, rather than in processes or systems. Jobs emphasizes the importance of having a clear vision of the future and not being afraid to take bold steps to achieve it.\n",
      "\n",
      "He also mentions that he was fired from Apple, but this event ultimately led to his return to the company and the success of NeXT, which was later acquired by Apple. This suggests that Jobs believes in second chances and the ability to learn from mistakes.\n",
      "\n",
      "Overall, Steve Jobs' management style is characterized as innovative, intuitive, and people-focused, rather than being overly concerned with processes or systems. He prioritizes creativity, vision, and taking risks, which contributed to his success in the technology industry.\n"
     ]
    }
   ],
   "source": [
    "# APPROACH 1: only PDF as a source\n",
    "# 5. Ask a question\n",
    "query = \"What can you say about Steve Jobs style of management?\"\n",
    "result = pdf_qa.invoke({\"query\": query})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0832b479-1d35-47a3-b9f4-a78e7b70f6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPROACH 2: FULL VERSION - Three Sources - the fully blown approach: leverage PDFs + LLM + Internet\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "#from langchain.tools import SerpAPIWrapper # Import SerpAPIWrapper explicitly\n",
    "import os\n",
    "\n",
    "# 1. Load from PDFs\n",
    "# This shows how to generalize to a list of pdfs :\n",
    "''' pdf_paths = [\"path/to/pdf1.pdf\", \"path/to/pdf2.pdf\"]  # Replace with your PDF paths\n",
    "pdf_docs = []\n",
    "for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pdf_docs.extend(loader.load())\n",
    "'''\n",
    "# 2. Split text from PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 3. Create vectorstore from PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 4. Initialize LLM (for both internal knowledge and final answer generation)\n",
    "# implemented above in the shared section of the notebook so no need to repeat code here\n",
    "\n",
    "# 5. Initialize the internet searches - SerpAPI - a tool for internet search (Crucially with the wrapper)\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "serpapi_api_key = os.environ.get(\"SERPAPI_API_KEY\") #does not work, so workaround below\n",
    "SERPAPI_API_KEY='49c6ecf880ecddcca49c7795464d7d235a932823cba6f66afe70c01e91383536'\n",
    "serpapi_api_key=SERPAPI_API_KEY\n",
    "if serpapi_api_key:\n",
    "    internet_search = SerpAPIWrapper(serpapi_api_key=serpapi_api_key)\n",
    "else:\n",
    "    print(\"SERPAPI_API_KEY environment variable not set. Internet searches will be unavailable.\")\n",
    "    internet_search = [] # search is unavailable if no API key\n",
    "\n",
    "# 6. Define RetrievalQA chain for PDFs\n",
    "# implemented above in the shared section of the notebook so no need to repeat code her\n",
    "#QA_PROMPT = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "#pdf_qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=pdf_retriever, chain_type_kwargs={\"prompt\": QA_PROMPT})\n",
    "\n",
    "\n",
    "def get_context(query):\n",
    "    pdf_context = pdf_qa.invoke({\"query\": query})['result']\n",
    "#    print(f\"PDF context: {pdf_context}\")\n",
    "    llm_context = llm.invoke(query).content\n",
    "#    print(f\"LLM context: {llm_context}\")\n",
    "    internet_context = internet_search.run(query) if internet_search else \"Internet search is unavailable.\" # Direct SerpAPI use\n",
    "#    print(f\"Internet context: {internet_context}\")\n",
    "    combined_context = f\"PDF Context:\\n{pdf_context}\\n\\nLLM Context:\\n{llm_context}\\n\\nInternet Context:\\n{internet_context}\"\n",
    "#    print(combined_context)\n",
    "    return combined_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cd5e92b-e67f-4e76-9438-57f94afa1a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6w/77j1g1r91fqfc5x3qqx3mrdw0000gn/T/ipykernel_24250/33831078.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  final_chain = LLMChain(llm=llm, prompt=FINAL_PROMPT)\n"
     ]
    }
   ],
   "source": [
    "# 8. Final prompt template and chain\n",
    "final_template = \"\"\"Use the combined context below from PDFs, an LLM, and the internet to answer the question. Treat the three contexts as three sources of information. If the information is not present in the context say I don't know.\n",
    "\n",
    "Combined Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "FINAL_PROMPT = PromptTemplate(\n",
    "    template=final_template, input_variables=[\"combined_context\", \"question\"]\n",
    ")\n",
    "\n",
    "final_chain = LLMChain(llm=llm, prompt=FINAL_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d76100-9d8a-42ae-9d24-3aed514f4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ask a question\n",
    "query = \"How many children did Steve Jobs have?\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2bfa9d5-0da2-4013-b2ae-858c46dfda5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the combined context provided, here is what I know about Steve Jobs:\n",
      "\n",
      "Steve Jobs (1955-2011) was an American businessman, inventor, and investor who co-founded Apple Inc. and Pixar Animated Studios. He was born in San Francisco, California, and grew up in the Bay Area. Jobs showed an early interest in electronics and design, and he dropped out of college to pursue his passion for technology.\n",
      "\n",
      "Jobs co-founded Apple in 1976 with Steve Wozniak, and the company's first product, the Apple I, was a personal computer that Jobs helped design and market. He later acquired Pixar Animation Studios in 1986, where he served as CEO until it was acquired by Disney in 2006.\n",
      "\n",
      "Under Jobs' leadership, Apple created some of the most successful and beloved electronic devices, including the Macintosh computer, the iPod, the iPhone, and the iPad. His design philosophy emphasized simplicity, elegance, and user experience, and his influence can be seen in everything from smartphones to tablets to medical devices.\n",
      "\n",
      "Jobs was known for his charismatic personality, keen insights into human behavior, and ability to merge technology and art. He was a master showman who captivated audiences with his product launches and presentations. Despite facing personal challenges, including his struggle with cancer, Jobs remained an innovative and influential figure in the world of technology until his death in 2011.\n",
      "\n",
      "Overall, Steve Jobs was a visionary entrepreneur, inventor, and designer who transformed the world of technology and beyond. His legacy continues to inspire and shape the culture and society we live in today.\n"
     ]
    }
   ],
   "source": [
    "# 9. Ask a question\n",
    "query = \"Who was Steve Jobs?\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ff9c1-a903-4f27-bb52-f3a27beb7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Ask a question\n",
    "query = \"What can you say about Steve Jobs style of management?\"\n",
    "combined_context = get_context(query)\n",
    "final_answer = final_chain.invoke({\"combined_context\": combined_context, \"question\": query})\n",
    "print(final_answer[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7df16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
